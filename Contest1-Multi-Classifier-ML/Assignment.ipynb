{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0bo-u-yFqyR"
      },
      "source": [
        "**Members of the group:**\n",
        "\n",
        "Antoniello Antonia\n",
        "\n",
        "Casale Teresa\n",
        "\n",
        "Cerino Mario\n",
        "\n",
        "Palladino Amedeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5YzteYL_RTp",
        "outputId": "7bce7210-1472-490b-88ff-9e1875f749b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PLxEUVLBjzN"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Machine Learning/Exercises2019/Exercise')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4CfAISoGIYz"
      },
      "source": [
        "We import all the libraries we need for our classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR8QH6NrBlh3",
        "outputId": "3e0b4294-985c-4d0b-8632-1db64fef8249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import h5py\n",
        "import pickle\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt');\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ-CTtLmGNSX"
      },
      "source": [
        "The following function takes in input a 1D vector and in output returns a 2D vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM0RYDeKB6n5"
      },
      "source": [
        "def to2D(x):\n",
        "    return np.reshape(x,(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3QG-HqYCIwX"
      },
      "source": [
        "The following class is used to extract features in a format supported by machine learning algorithms from textual datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS83Fu0nCM9u"
      },
      "source": [
        "class FeatureExtractorText:\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        features = self._feature_extraction(X)\n",
        "        container._features = features\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        self.fit(X,None)\n",
        "        return X\n",
        "\n",
        "    \n",
        "    def _feature_extraction(self, X):\n",
        "        corpus = X\n",
        "        doc_word_count = np.array([len(row)  for row in corpus])\n",
        "        doc_char_count = np.array([ np.array([ len(word) for word in row]).sum()  for row in corpus])\n",
        "        word_density = doc_char_count/(doc_word_count + 1)\n",
        "        f1 = np.concatenate((to2D(doc_word_count), to2D(doc_char_count)), axis=1)\n",
        "        features = np.concatenate((f1,to2D(word_density)), axis=1)\n",
        "        return features\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNUBrv4wCaJV"
      },
      "source": [
        "The following class is used to join the vectorized data with the container features. In order to work, Feature Extractor requires a set of non-vectorized documents. After extracting the features, we save them in a container for later use in the pipeline; in particular, the joiner takes the features from the container and joins them to the features processed by Tfidf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZaEgCaECWhm"
      },
      "source": [
        "class Joiner:\n",
        "    \n",
        "      \n",
        "    def fit(self, X, y):\n",
        "        self.X_ = X.toarray()\n",
        "        self.features = container._features\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        self_new = self.fit(X,None)\n",
        "        X_, features = self_new.X_, self_new.features\n",
        "        X_tr = np.concatenate((self.X_,self.features), axis=1)\n",
        "\n",
        "        return X_tr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0kVmo0FCuod"
      },
      "source": [
        "The following class is used as intermediate between the extractor and the joiner objects in the pipeline structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fazkSORgCsl5"
      },
      "source": [
        "class FeatureContainer:\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        self._features = None\n",
        "        \n",
        "    \n",
        "    def get_features(self):\n",
        "        \n",
        "        #sinchronized method\n",
        "        with threading.Lock():\n",
        "            return self._features\n",
        "    \n",
        "    def set_features(self, features):\n",
        "        \n",
        "\n",
        "        with threading.Lock():\n",
        "            self._features = features\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWbBoroHEhEP"
      },
      "source": [
        "Preprocessing phase: it provides to remove useless word from the text and to reduce the words to their root (prefix)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nCokYclEY0w"
      },
      "source": [
        "#method used for text splitting\n",
        "def tokenizer(text):\n",
        "    return text.split(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqJ730IMEdlQ"
      },
      "source": [
        "stop = stopwords.words('english')\n",
        "porter=PorterStemmer()\n",
        "def tokenizer_porter(text):\n",
        "    return[porter.stem(word) for word in text.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTJwk60yaJQA"
      },
      "source": [
        "porter=SnowballStemmer('english')\n",
        "def tokenizer_snowball(text):\n",
        "    return[porter.stem(word) for word in text.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhtjdtS6Gtv2"
      },
      "source": [
        "Loading of the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85v04WFkFLAe"
      },
      "source": [
        "f = h5py.File('data.h5', 'r')\n",
        "X_training = f['X_training'][:]\n",
        "y_training = f['y_training'][:]\n",
        "X_test = f['X_test'][:]\n",
        "y_test = f['y_test'][:]\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylh8S-CVFMrN"
      },
      "source": [
        "with open('feature.names','rb') as f:\n",
        "    vector = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns5TdapyG0WQ"
      },
      "source": [
        "Function used to convert vectorized data into documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyvsqrU_FT8l"
      },
      "source": [
        "def retrieve_text(X_, vector):\n",
        "    X_training_str = []\n",
        "    for doc in X_:\n",
        "        str_doc = \"\"\n",
        "        for i,item in enumerate(doc):\n",
        "            if item>0:\n",
        "                str_doc += (vector[i]+\" \")\n",
        "        \n",
        "        X_training_str.append(str_doc)\n",
        "    \n",
        "    return X_training_str\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYyoKX5CG567"
      },
      "source": [
        "We split the data set in training set and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBAdEUzoFXI2"
      },
      "source": [
        "X_training, X_test = retrieve_text(X_training, vector), retrieve_text(X_test, vector)\n",
        "X_training = np.array(X_training)\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62KT3BGGFpPd"
      },
      "source": [
        "Pipeline use for all the following classifiers.\n",
        "\n",
        "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQfUwrocHLUm"
      },
      "source": [
        "## **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjIi06ACuIG",
        "outputId": "b3b926b7-c7b7-44ab-f085-04539abebc98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                       lowercase=False,\n",
        "                       preprocessor=None,\n",
        "                       ngram_range=(1,2),\n",
        "                       stop_words=stop,\n",
        "                       tokenizer=tokenizer_snowball,\n",
        "                      \n",
        "                       )\n",
        "param_grid = [\n",
        "             {\n",
        "                 'clf__C':[0.2,0.5, 1, 5,8, 10, 15, 20, 30, 40, 70, 100, 1000]\n",
        "              }]\n",
        "\n",
        "container = FeatureContainer()\n",
        "extractor = FeatureExtractorText()\n",
        "joiner = Joiner()\n",
        "\n",
        "lr_tfidf = Pipeline([('extractor', extractor),\n",
        "                     ('vect', tfidf),\n",
        "                     ('joiner', joiner),\n",
        "                     ('clf', LogisticRegression(random_state=0))])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=10)\n",
        "gs_lr_tfidf.fit(X_training, y_training)\n",
        "\n",
        "\n",
        "#Save the Classifier\n",
        "f=open('logistic.model', 'wb')\n",
        "pickle.dump(gs_lr_tfidf, f)\n",
        "f.close()\n",
        "\n",
        "print('Training set score =', gs_lr_tfidf.score(X_training, y_training))\n",
        "print('Test set score =', gs_lr_tfidf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   29.9s\n",
            "[Parallel(n_jobs=10)]: Done  65 out of  65 | elapsed:   52.8s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training set score = 0.9965004374453194\n",
            "Test set score = 0.8765306122448979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5xU1vPhHeDY"
      },
      "source": [
        "## **Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlYMl8TG7Lj8",
        "outputId": "3a49965e-a57d-4871-8dd9-f09918a4ff0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "tfidf = CountVectorizer(strip_accents=None,\n",
        "                       lowercase=False,\n",
        "                       preprocessor=None,\n",
        "                       ngram_range=(1,2),\n",
        "                       tokenizer=tokenizer_snowball)\n",
        "\n",
        "#parameters setting for NB classifier:\n",
        "param_grid = [{\n",
        "               'clf__alpha':[0,0.4,0.7,0.8,1,5]\n",
        "              }]\n",
        "\n",
        "#definition of the components to insert in the pipeline\n",
        "container = FeatureContainer()\n",
        "extractor = FeatureExtractorText()\n",
        "joiner = Joiner()\n",
        "\n",
        "#pipeline definition\n",
        "lr_tfidf3 = Pipeline([\n",
        "                      ('vect', tfidf),\n",
        "                      ('clf',MultinomialNB())])\n",
        "\n",
        "gs_lr_tfidf3 = GridSearchCV(lr_tfidf3, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=8)\n",
        "multinomial_clf = gs_lr_tfidf3.fit(X_training, y_training)\n",
        "\n",
        "#Save the Classifier\n",
        "f=open('nbclassifier.model', 'wb')\n",
        "pickle.dump(multinomial_clf , f)\n",
        "f.close()\n",
        "\n",
        "print('Training set score =', multinomial_clf.score(X_training, y_training))\n",
        "print('Test set score =', multinomial_clf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  30 out of  30 | elapsed:   18.1s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training set score = 0.9881889763779528\n",
            "Test set score = 0.8438775510204082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfvKVPieHxuP"
      },
      "source": [
        "## **Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v-cQhjWH0wq",
        "outputId": "7dfdd7e8-4053-46d5-b66d-438b0a46805f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                       lowercase=False,\n",
        "                       preprocessor=None,\n",
        "                       ngram_range=(1,2),\n",
        "                       stop_words=None,\n",
        "                       tokenizer=tokenizer_snowball,\n",
        "                       norm=None,\n",
        "                       use_idf=True)\n",
        "\n",
        "#parameters setting for Decision Tree classifier:\n",
        "param_grid = [{\n",
        "               'clf__max_depth':[5],\n",
        "                'clf__max_leaf_nodes':[13]      \n",
        "\n",
        "              }]\n",
        "\n",
        "#definition of the components to insert in the pipeline\n",
        "container = FeatureContainer()\n",
        "extractor = FeatureExtractorText()\n",
        "joiner = Joiner()\n",
        "\n",
        "#pipeline definition\n",
        "lr_tfidf2 = Pipeline([('extractor', extractor),\n",
        "                      ('vect', tfidf),\n",
        "                      ('joiner', joiner),\n",
        "                      ('clf', DecisionTreeClassifier(random_state=0))])\n",
        "gs_lr_tfidf2 = GridSearchCV(lr_tfidf2, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=8)\n",
        "\n",
        "gs_lr_tfidf2.fit(X_training, y_training)\n",
        "gs_lr_tfidf2.score(X_test, y_test)\n",
        "decision_tree = gs_lr_tfidf2\n",
        "\n",
        "#Save the Classifier\n",
        "f=open('decision_tree.model', 'wb')\n",
        "pickle.dump(decision_tree, f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "print('Training set score=',decision_tree.score(X_training, y_training))\n",
        "print('Test set score=',decision_tree.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done   2 out of   5 | elapsed:   19.8s remaining:   29.7s\n",
            "[Parallel(n_jobs=8)]: Done   5 out of   5 | elapsed:   20.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training set score= 0.8613298337707787\n",
            "Test set score= 0.863265306122449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___TL6dnH9QR"
      },
      "source": [
        "## **Multiclassification combiner with weighted voting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTidCZu-H8XI"
      },
      "source": [
        "class Combiner:\n",
        "    \n",
        "    def __init__(self, classifiers,):\n",
        "        self.classifiers_ = classifiers\n",
        "        \n",
        "    #computation of reliability vectors for each classifier\n",
        "    def compute_reliability(self, X, y):\n",
        "        X_0 = X[y.ravel()==0]\n",
        "        X_1 = X[y.ravel() == 1]\n",
        "        n_samples_0 = X_0.shape[0]\n",
        "        n_samples_1 = X_1.shape[0]\n",
        "        reliability_dict = {}\n",
        "        \n",
        "        for clf in self.classifiers_:\n",
        "            \n",
        "            y_pred_0 = clf.predict(X_0.ravel())\n",
        "            correct_0 = y_pred_0[y_pred_0==0].shape[0]\n",
        "            reliability_0 = correct_0/n_samples_0\n",
        "            \n",
        "            y_pred_1 = clf.predict(X_1.ravel())\n",
        "            correct_1 = y_pred_1[y_pred_1==1].shape[0]\n",
        "            reliability_1 = correct_1/n_samples_1\n",
        "            \n",
        "            reliability_dict[clf] = [ reliability_0,  reliability_1]\n",
        "            \n",
        "        self.rel_dict = reliability_dict\n",
        "        \n",
        "        return reliability_dict\n",
        "      \n",
        " \n",
        "      \n",
        "    def compute_proba_predictions(self, X_test, Y_test):\n",
        "        \n",
        "        y = []\n",
        "        y_=[]\n",
        "        \n",
        "        for clf in self.classifiers_:\n",
        "          y_.append(clf.predict_proba(X_test).astype(np.int32))\n",
        "          \n",
        "        \n",
        "        \n",
        "        y_ = np.moveaxis(y_,(0,1,2), (1,0,2))\n",
        "        rel_dict = self.rel_dict\n",
        "        \n",
        "        for sample in y_:\n",
        "          vec = dict({0:0, 1:0})\n",
        "          for i,clf in enumerate(sample):\n",
        "            #clf[0] è il risultato della classe 0 del corrente classifier\n",
        "            vec[0] += clf[0]*self.rel_dict[self.classifiers_[i]][0]\n",
        "            vec[1] += clf[1]*self.rel_dict[self.classifiers_[i]][1]\n",
        "            \n",
        "          if vec[0] > vec[1]:\n",
        "              y.append(0)\n",
        "          else:\n",
        "              y.append(1)\n",
        "        return y\n",
        "\n",
        "\n",
        "    def compute_predictions(self, X_test, Y_test):\n",
        "        \n",
        "        y = []\n",
        "        y_=[]\n",
        "        \n",
        "        for clf in self.classifiers_:\n",
        "          y_.append(clf.predict(X_test).astype(np.int32))\n",
        "          \n",
        "        \n",
        "        \n",
        "        y_ = np.array(y_).T\n",
        "        rel_dict = self.rel_dict\n",
        "        \n",
        "        for y_el in y_:\n",
        "          \n",
        "          vec = dict({0:0, 1:0})\n",
        "\n",
        "          \n",
        "          for index,i in enumerate(y_el):\n",
        "            \n",
        "\n",
        "            if vec[i]==0:\n",
        "                vec[i] = rel_dict[self.classifiers_[index]][i]\n",
        "            else:\n",
        "                vec[i]+=rel_dict[self.classifiers_[index]][i]\n",
        "\n",
        "          if vec[0] > vec[1]:\n",
        "              y.append(0)\n",
        "          else:\n",
        "              y.append(1)\n",
        "              \n",
        "        return y\n",
        "\n",
        "      \n",
        "    def score(self,y_comb, y_test): \n",
        "          y_comb = np.array(y_comb)\n",
        "          return 1-(np.abs(y_test.ravel() - y_comb).sum()/y_test.shape[0])\n",
        "          \n",
        "        \n",
        "      \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW8Vp-MKJ4qK",
        "outputId": "b30405a8-a8d6-4dc0-9fc5-3b59772242f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Data set division between training set (70%) and test set (30%).\n",
        "X_training_1, X_test_1, y_training_1, y_test_1 = train_test_split(X_training, y_training, test_size=0.3, shuffle=True, random_state=11)\n",
        "\n",
        "#definition of the involved classifiers\n",
        "estimators = [gs_lr_tfidf,decision_tree, multinomial_clf]\n",
        "\n",
        "combiner = Combiner(estimators)\n",
        "\n",
        "#definition of the reliability vectors\n",
        "combiner.compute_reliability(X_training_1, y_training_1)\n",
        "\n",
        "#combiner execution\n",
        "X_comb=combiner.compute_predictions(X_training,y_training)\n",
        "y_comb=combiner.compute_predictions(X_test,y_test)\n",
        "\n",
        "#Save the Combiner\n",
        "comb = np.array([combiner.score(X_comb,y_training),combiner.score(y_comb,y_test)])\n",
        "f=open('combiner.model', 'wb')\n",
        "pickle.dump(comb, f)\n",
        "f.close()\n",
        "\n",
        "#Final performance evaluation\n",
        "print('Multiclassification Combiner score on training set =',combiner.score(X_comb,y_training))\n",
        "print('Multiclassification Combiner score on test set =',combiner.score(y_comb,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multiclassification Combiner score on training set = 0.994750656167979\n",
            "Multiclassification Combiner score on test set = 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}